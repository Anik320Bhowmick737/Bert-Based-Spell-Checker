{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8879999,"sourceType":"datasetVersion","datasetId":5344392},{"sourceId":8914997,"sourceType":"datasetVersion","datasetId":5360132},{"sourceId":8918363,"sourceType":"datasetVersion","datasetId":5363567}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft\n!pip install evaluate\n!pip install seqeval\nfrom transformers import (AutoModelForSequenceClassification, BertForTokenClassification, AutoTokenizer,\n                          TrainingArguments, Trainer, DataCollatorForTokenClassification,\n                         TrainingArguments,Trainer)\nimport pandas as pd\nimport evaluate\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom peft import LoraModel, LoraConfig, get_peft_model\nseqeval = evaluate.load(\"seqeval\")\nBERT = BertForTokenClassification.from_pretrained('google-bert/bert-base-cased',num_labels = 2)\nTokenizer =  AutoTokenizer.from_pretrained('google-bert/bert-base-cased')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-11T11:25:03.077349Z","iopub.execute_input":"2024-07-11T11:25:03.077628Z","iopub.status.idle":"2024-07-11T11:26:12.967762Z","shell.execute_reply.started":"2024-07-11T11:25:03.077604Z","shell.execute_reply":"2024-07-11T11:26:12.966949Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=13badebf1a5eba0ed26abd395d677fb95070c6db2ca62b72e7c301d98ad4d946\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"},{"name":"stderr","text":"2024-07-11 11:25:55.990370: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-11 11:25:55.990520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-11 11:25:56.167525: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48793de30c474b2d800bd3cb5bf50e13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baaf7456b1d24f53b1aed1e199503fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ae55e5160746e099427e2b73c0de41"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba2513b307e64920a0882855cffffce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bc50acae3044197961200f19800761d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b05476e7b364f4bbc94e8980326991b"}},"metadata":{}}]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet, words\nfrom nltk.tokenize import regexp_tokenize\npattern = r\"\\b\\w+(?:'\\w+)?\\b\"\nimport random\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('words')\n\ndef label_generator(txt):\n    c = word_tokenize(txt[0])\n    i = word_tokenize(txt[1])\n    label = []\n    for w1,w2 in zip(c,i):\n        if w1!=w2:\n            label.append(1)\n        else:\n            label.append(0)\n\n    return label","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:26:12.969522Z","iopub.execute_input":"2024-07-11T11:26:12.970104Z","iopub.status.idle":"2024-07-11T11:26:13.764949Z","shell.execute_reply.started":"2024-07-11T11:26:12.970075Z","shell.execute_reply":"2024-07-11T11:26:13.764061Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package words to /usr/share/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\ncontractions = {\n    \"I'll\": [\"isle\", \"aisle\"],\n    \"we'll\": [\"weal\", \"wheel\"],\n    \"he'll\": [\"heal\", \"heel\"],\n    \"you'll\": [\"yule\"],\n    \"aren't\": [\"aunt\"],\n    \"he'd\": [\"heed\"],\n    \"it's\": [\"its\"],\n    \"they're\": [\"their\", \"there\"],\n    \"who's\": [\"whose\"],\n    \"we'd\" : [\"weed\"],\n    \"you're\": [\"yore\",\"yaw\",\"your\"],\n    \"cannot\": [\"a\"],\n    \"daydreams'\":[\"daydreamed\"]\n}\n\n\n# Replace contractions in Data[\"Errors\"]\nfor original, replacements in contractions.items():\n    Data[\"Errors\"] = Data[\"Errors\"].str.replace(original, random.choice(replacements))","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:23:42.950228Z","iopub.execute_input":"2024-07-10T05:23:42.950811Z","iopub.status.idle":"2024-07-10T05:23:43.628745Z","shell.execute_reply.started":"2024-07-10T05:23:42.950752Z","shell.execute_reply":"2024-07-10T05:23:43.627379Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"Data = pd.read_csv(\"/kaggle/input/spellcheckdataset/Final Training dataset_1.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:23:41.775916Z","iopub.execute_input":"2024-07-10T05:23:41.776342Z","iopub.status.idle":"2024-07-10T05:23:42.692222Z","shell.execute_reply.started":"2024-07-10T05:23:41.776308Z","shell.execute_reply":"2024-07-10T05:23:42.690738Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"#Data = pd.read_csv(\"/kaggle/input/spell-check-data/Final Training dataset.csv\")\nfrom tqdm import tqdm\n\ntqdm.pandas(desc=\"Processing rows\")\n\nData[\"Label\"] = Data[['Final','Errors']].progress_apply(label_generator,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:23:47.335905Z","iopub.execute_input":"2024-07-10T05:23:47.336790Z","iopub.status.idle":"2024-07-10T05:26:01.771852Z","shell.execute_reply.started":"2024-07-10T05:23:47.336728Z","shell.execute_reply":"2024-07-10T05:26:01.770501Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stderr","text":"Processing rows:   0%|          | 0/65735 [00:00<?, ?it/s]/tmp/ipykernel_33/969590983.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  c = word_tokenize(txt[0])\n/tmp/ipykernel_33/969590983.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  i = word_tokenize(txt[1])\nProcessing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65735/65735 [02:14<00:00, 489.06it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"Data[\"Label\"].loc[1313]","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:37:19.046631Z","iopub.execute_input":"2024-07-06T10:37:19.047025Z","iopub.status.idle":"2024-07-06T10:37:19.057227Z","shell.execute_reply.started":"2024-07-06T10:37:19.046994Z","shell.execute_reply":"2024-07-06T10:37:19.056060Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0]"},"metadata":{}}]},{"cell_type":"code","source":"# remove if the mask is more than 50 %, Useful information can not be obtained from them\nindex = []\nfor i,items in enumerate(Data[\"Label\"].values):\n    if(sum(items)/len(items)>0.5):\n        index.append(i)\n        \nData.drop(index,inplace=True)\nData.reset_index(drop=True,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:26:01.773796Z","iopub.execute_input":"2024-07-10T05:26:01.774268Z","iopub.status.idle":"2024-07-10T05:26:01.896954Z","shell.execute_reply.started":"2024-07-10T05:26:01.774217Z","shell.execute_reply":"2024-07-10T05:26:01.895743Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# Calculate consecutive ones maxlen subarray. If more than 3 trim off. because context will be lost completely\nconsec_ones = []\nfor i,items in enumerate(Data[\"Label\"].values):\n    max_len = 0\n    counter = 0\n    for j in items:\n        if j==1:\n            counter+=1\n        else:\n            max_len = max(max_len,counter)\n            counter=0\n    max_len = max(max_len,counter)\n    consec_ones.append(max_len)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:26:01.898283Z","iopub.execute_input":"2024-07-10T05:26:01.898657Z","iopub.status.idle":"2024-07-10T05:26:03.416161Z","shell.execute_reply.started":"2024-07-10T05:26:01.898624Z","shell.execute_reply":"2024-07-10T05:26:03.414969Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"import numpy as np\narr = np.array(consec_ones)\nindices = np.where(arr > 3)[0]\nprint(len(indices))\n#print(indices)\nData.drop(indices,inplace = True)\nData.reset_index(inplace = True,drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:26:03.418886Z","iopub.execute_input":"2024-07-10T05:26:03.419311Z","iopub.status.idle":"2024-07-10T05:26:03.453821Z","shell.execute_reply.started":"2024-07-10T05:26:03.419272Z","shell.execute_reply":"2024-07-10T05:26:03.452276Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"4507\n","output_type":"stream"}]},{"cell_type":"code","source":"Train_Data, Test_Data = train_test_split(Data,test_size=0.2)\nTrain_Data.reset_index(inplace=True,drop=True)\nTest_Data.reset_index(inplace=True,drop=True)\nTrain_data = Train_Data[:40000]\nValidation_data = Train_Data[40000:]\n\nText_train = Train_data[\"Errors\"].values.tolist()\nLabel_train = Train_data[\"Label\"].values.tolist()\n\nText_val = Validation_data[\"Errors\"].values.tolist()\nLabel_val = Validation_data[\"Label\"].values.tolist()\n\nText_test = Test_Data[\"Errors\"].values.tolist()\nLabel_test = Test_Data[\"Label\"].values.tolist()\n\ndata_dict = {\"Text\":Text_train,\"Label\":Label_train}\nds_train = Dataset.from_dict(data_dict)\n\ndata_dict = {\"Text\":Text_val,\"Label\":Label_val}\nds_val = Dataset.from_dict(data_dict)\n\ndata_dict = {\"Text\":Text_test,\"Label\":Label_test}\nds_test = Dataset.from_dict(data_dict)\n\ndataset = DatasetDict({\n    'train': ds_train,\n    'test': ds_test,\n    'validation': ds_val\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:26:03.455390Z","iopub.execute_input":"2024-07-10T05:26:03.456137Z","iopub.status.idle":"2024-07-10T05:26:04.566609Z","shell.execute_reply.started":"2024-07-10T05:26:03.456102Z","shell.execute_reply":"2024-07-10T05:26:04.564975Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:26:04.569976Z","iopub.execute_input":"2024-07-10T05:26:04.570485Z","iopub.status.idle":"2024-07-10T05:26:04.581652Z","shell.execute_reply.started":"2024-07-10T05:26:04.570429Z","shell.execute_reply":"2024-07-10T05:26:04.579741Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Text', 'Label'],\n        num_rows: 40000\n    })\n    test: Dataset({\n        features: ['Text', 'Label'],\n        num_rows: 12231\n    })\n    validation: Dataset({\n        features: ['Text', 'Label'],\n        num_rows: 8920\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"#count = 0\ndef tokenize_and_align_labels(example):\n    #global count\n    \n    tokenized_text = word_tokenize(example[\"Text\"])\n    tokenized_inputs = Tokenizer(tokenized_text, truncation=True ,is_split_into_words=True)\n\n    label = example[\"Label\"]\n\n    # Map tokens to their respective word indices\n    word_ids = tokenized_inputs.word_ids()\n    \n    previous_word_idx = None\n    label_ids = []\n\n    # Align labels with tokenized inputs\n    for word_idx in word_ids:\n        if word_idx is None:\n            label_ids.append(-100)\n        elif word_idx != previous_word_idx:\n            label_ids.append(label[word_idx])\n        else:\n            label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    tokenized_inputs[\"labels\"] = label_ids\n\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:26:13.765972Z","iopub.execute_input":"2024-07-11T11:26:13.766279Z","iopub.status.idle":"2024-07-11T11:26:13.773319Z","shell.execute_reply.started":"2024-07-11T11:26:13.766255Z","shell.execute_reply":"2024-07-11T11:26:13.772421Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"new = dataset.map(tokenize_and_align_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:26:13.775569Z","iopub.execute_input":"2024-07-11T11:26:13.776149Z","iopub.status.idle":"2024-07-11T11:26:14.535626Z","shell.execute_reply.started":"2024-07-11T11:26:13.776115Z","shell.execute_reply":"2024-07-11T11:26:14.534385Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mmap(tokenize_and_align_labels)\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"],"ename":"NameError","evalue":"name 'dataset' is not defined","output_type":"error"}]},{"cell_type":"code","source":"new","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:27:57.537920Z","iopub.execute_input":"2024-07-10T05:27:57.538319Z","iopub.status.idle":"2024-07-10T05:27:57.548502Z","shell.execute_reply.started":"2024-07-10T05:27:57.538277Z","shell.execute_reply":"2024-07-10T05:27:57.546785Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Text', 'Label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 40000\n    })\n    test: Dataset({\n        features: ['Text', 'Label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 12231\n    })\n    validation: Dataset({\n        features: ['Text', 'Label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 8920\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"label_map = {0: 'CORRECT', 1: 'INCORRECT'}\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_map[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_map[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:26:39.935806Z","iopub.execute_input":"2024-07-11T11:26:39.936194Z","iopub.status.idle":"2024-07-11T11:26:39.944177Z","shell.execute_reply.started":"2024-07-11T11:26:39.936166Z","shell.execute_reply":"2024-07-11T11:26:39.943116Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer=Tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:26:41.786363Z","iopub.execute_input":"2024-07-11T11:26:41.786718Z","iopub.status.idle":"2024-07-11T11:26:41.791024Z","shell.execute_reply.started":"2024-07-11T11:26:41.786690Z","shell.execute_reply":"2024-07-11T11:26:41.789956Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Freezing the layers","metadata":{}},{"cell_type":"code","source":"BERT = BertForTokenClassification.from_pretrained('google-bert/bert-base-cased',num_labels = 2)\n# Useless when using PEFT as it automatically freezes these weights\nfor params in BERT.bert.parameters():\n    params.requires_grad = False\n    \nfor params in BERT.bert.encoder.layer[0].parameters():\n    params.requires_grad = True\n    \n\"\"\"for params in BERT.bert.encoder.layer[5].parameters():\n    params.requires_grad = True\"\"\"\n    \nfor params in BERT.bert.encoder.layer[11].parameters():\n    params.requires_grad = True\n    \nfor name, param in BERT.named_parameters():\n    print(f\"{name}: requires_grad = {param.requires_grad}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:27:57.581293Z","iopub.execute_input":"2024-07-10T05:27:57.581661Z","iopub.status.idle":"2024-07-10T05:27:58.088883Z","shell.execute_reply.started":"2024-07-10T05:27:57.581631Z","shell.execute_reply":"2024-07-10T05:27:58.087147Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":114,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"bert.embeddings.word_embeddings.weight: requires_grad = False\nbert.embeddings.position_embeddings.weight: requires_grad = False\nbert.embeddings.token_type_embeddings.weight: requires_grad = False\nbert.embeddings.LayerNorm.weight: requires_grad = False\nbert.embeddings.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.0.attention.self.query.weight: requires_grad = True\nbert.encoder.layer.0.attention.self.query.bias: requires_grad = True\nbert.encoder.layer.0.attention.self.key.weight: requires_grad = True\nbert.encoder.layer.0.attention.self.key.bias: requires_grad = True\nbert.encoder.layer.0.attention.self.value.weight: requires_grad = True\nbert.encoder.layer.0.attention.self.value.bias: requires_grad = True\nbert.encoder.layer.0.attention.output.dense.weight: requires_grad = True\nbert.encoder.layer.0.attention.output.dense.bias: requires_grad = True\nbert.encoder.layer.0.attention.output.LayerNorm.weight: requires_grad = True\nbert.encoder.layer.0.attention.output.LayerNorm.bias: requires_grad = True\nbert.encoder.layer.0.intermediate.dense.weight: requires_grad = True\nbert.encoder.layer.0.intermediate.dense.bias: requires_grad = True\nbert.encoder.layer.0.output.dense.weight: requires_grad = True\nbert.encoder.layer.0.output.dense.bias: requires_grad = True\nbert.encoder.layer.0.output.LayerNorm.weight: requires_grad = True\nbert.encoder.layer.0.output.LayerNorm.bias: requires_grad = True\nbert.encoder.layer.1.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.1.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.1.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.1.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.1.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.1.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.1.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.1.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.1.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.1.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.1.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.1.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.1.output.dense.weight: requires_grad = False\nbert.encoder.layer.1.output.dense.bias: requires_grad = False\nbert.encoder.layer.1.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.1.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.2.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.2.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.2.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.2.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.2.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.2.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.2.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.2.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.2.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.2.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.2.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.2.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.2.output.dense.weight: requires_grad = False\nbert.encoder.layer.2.output.dense.bias: requires_grad = False\nbert.encoder.layer.2.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.2.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.3.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.3.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.3.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.3.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.3.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.3.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.3.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.3.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.3.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.3.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.3.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.3.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.3.output.dense.weight: requires_grad = False\nbert.encoder.layer.3.output.dense.bias: requires_grad = False\nbert.encoder.layer.3.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.3.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.4.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.4.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.4.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.4.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.4.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.4.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.4.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.4.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.4.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.4.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.4.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.4.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.4.output.dense.weight: requires_grad = False\nbert.encoder.layer.4.output.dense.bias: requires_grad = False\nbert.encoder.layer.4.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.4.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.5.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.5.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.5.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.5.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.5.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.5.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.5.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.5.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.5.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.5.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.5.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.5.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.5.output.dense.weight: requires_grad = False\nbert.encoder.layer.5.output.dense.bias: requires_grad = False\nbert.encoder.layer.5.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.5.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.6.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.6.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.6.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.6.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.6.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.6.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.6.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.6.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.6.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.6.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.6.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.6.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.6.output.dense.weight: requires_grad = False\nbert.encoder.layer.6.output.dense.bias: requires_grad = False\nbert.encoder.layer.6.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.6.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.7.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.7.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.7.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.7.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.7.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.7.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.7.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.7.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.7.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.7.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.7.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.7.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.7.output.dense.weight: requires_grad = False\nbert.encoder.layer.7.output.dense.bias: requires_grad = False\nbert.encoder.layer.7.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.7.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.8.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.8.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.8.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.8.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.8.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.8.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.8.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.8.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.8.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.8.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.8.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.8.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.8.output.dense.weight: requires_grad = False\nbert.encoder.layer.8.output.dense.bias: requires_grad = False\nbert.encoder.layer.8.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.8.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.9.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.9.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.9.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.9.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.9.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.9.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.9.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.9.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.9.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.9.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.9.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.9.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.9.output.dense.weight: requires_grad = False\nbert.encoder.layer.9.output.dense.bias: requires_grad = False\nbert.encoder.layer.9.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.9.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.10.attention.self.query.weight: requires_grad = False\nbert.encoder.layer.10.attention.self.query.bias: requires_grad = False\nbert.encoder.layer.10.attention.self.key.weight: requires_grad = False\nbert.encoder.layer.10.attention.self.key.bias: requires_grad = False\nbert.encoder.layer.10.attention.self.value.weight: requires_grad = False\nbert.encoder.layer.10.attention.self.value.bias: requires_grad = False\nbert.encoder.layer.10.attention.output.dense.weight: requires_grad = False\nbert.encoder.layer.10.attention.output.dense.bias: requires_grad = False\nbert.encoder.layer.10.attention.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.10.attention.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.10.intermediate.dense.weight: requires_grad = False\nbert.encoder.layer.10.intermediate.dense.bias: requires_grad = False\nbert.encoder.layer.10.output.dense.weight: requires_grad = False\nbert.encoder.layer.10.output.dense.bias: requires_grad = False\nbert.encoder.layer.10.output.LayerNorm.weight: requires_grad = False\nbert.encoder.layer.10.output.LayerNorm.bias: requires_grad = False\nbert.encoder.layer.11.attention.self.query.weight: requires_grad = True\nbert.encoder.layer.11.attention.self.query.bias: requires_grad = True\nbert.encoder.layer.11.attention.self.key.weight: requires_grad = True\nbert.encoder.layer.11.attention.self.key.bias: requires_grad = True\nbert.encoder.layer.11.attention.self.value.weight: requires_grad = True\nbert.encoder.layer.11.attention.self.value.bias: requires_grad = True\nbert.encoder.layer.11.attention.output.dense.weight: requires_grad = True\nbert.encoder.layer.11.attention.output.dense.bias: requires_grad = True\nbert.encoder.layer.11.attention.output.LayerNorm.weight: requires_grad = True\nbert.encoder.layer.11.attention.output.LayerNorm.bias: requires_grad = True\nbert.encoder.layer.11.intermediate.dense.weight: requires_grad = True\nbert.encoder.layer.11.intermediate.dense.bias: requires_grad = True\nbert.encoder.layer.11.output.dense.weight: requires_grad = True\nbert.encoder.layer.11.output.dense.bias: requires_grad = True\nbert.encoder.layer.11.output.LayerNorm.weight: requires_grad = True\nbert.encoder.layer.11.output.LayerNorm.bias: requires_grad = True\nclassifier.weight: requires_grad = True\nclassifier.bias: requires_grad = True\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:31:01.980309Z","iopub.execute_input":"2024-07-11T11:31:01.980683Z","iopub.status.idle":"2024-07-11T11:31:01.986823Z","shell.execute_reply.started":"2024-07-11T11:31:01.980657Z","shell.execute_reply":"2024-07-11T11:31:01.985695Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",         \n    num_train_epochs=10,              \n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,    \n    warmup_steps=50,                \n    weight_decay=0.01,               \n    logging_dir=\"/kaggle/working/\",            \n    logging_steps=10,                \n    save_steps=500,                 \n    save_total_limit=3,              \n    eval_steps=500,                  \n    load_best_model_at_end=True, \n    evaluation_strategy = \"steps\",\n    report_to=[]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:27:58.100696Z","iopub.execute_input":"2024-07-10T05:27:58.101107Z","iopub.status.idle":"2024-07-10T05:27:58.116922Z","shell.execute_reply.started":"2024-07-10T05:27:58.101073Z","shell.execute_reply":"2024-07-10T05:27:58.115448Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=BERT,                     \n    args=training_args,               \n    train_dataset=new[\"train\"],\n    eval_dataset=new[\"validation\"],\n    tokenizer=Tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:08:59.448603Z","iopub.execute_input":"2024-07-10T06:08:59.449189Z","iopub.status.idle":"2024-07-10T06:08:59.470723Z","shell.execute_reply.started":"2024-07-10T06:08:59.449148Z","shell.execute_reply":"2024-07-10T06:08:59.469073Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:09:00.616036Z","iopub.execute_input":"2024-07-10T06:09:00.616474Z","iopub.status.idle":"2024-07-10T06:09:08.817802Z","shell.execute_reply.started":"2024-07-10T06:09:00.616439Z","shell.execute_reply":"2024-07-10T06:09:08.814659Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":121,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    3/50000 00:02 < 30:19:49, 0.46 it/s, Epoch 0.00/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2125\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2125\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=new[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T08:53:34.312812Z","iopub.execute_input":"2024-07-07T08:53:34.313114Z","iopub.status.idle":"2024-07-07T08:54:42.754037Z","shell.execute_reply.started":"2024-07-07T08:53:34.313088Z","shell.execute_reply":"2024-07-07T08:54:42.753065Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.03143584355711937,\n 'eval_precision': 0.9492524153689491,\n 'eval_recall': 0.9467598900146151,\n 'eval_f1': 0.9480045143367397,\n 'eval_accuracy': 0.990143401139718,\n 'eval_runtime': 68.4311,\n 'eval_samples_per_second': 130.35,\n 'eval_steps_per_second': 8.154,\n 'epoch': 10.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=new[\"train\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T08:54:53.471396Z","iopub.execute_input":"2024-07-07T08:54:53.471769Z","iopub.status.idle":"2024-07-07T09:00:03.934263Z","shell.execute_reply.started":"2024-07-07T08:54:53.471739Z","shell.execute_reply":"2024-07-07T09:00:03.933246Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.014110065996646881,\n 'eval_precision': 0.974600659594742,\n 'eval_recall': 0.9752701882043235,\n 'eval_f1': 0.9749353089512635,\n 'eval_accuracy': 0.9952984186447256,\n 'eval_runtime': 310.452,\n 'eval_samples_per_second': 128.844,\n 'eval_steps_per_second': 8.053,\n 'epoch': 10.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=new[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T09:00:03.936016Z","iopub.execute_input":"2024-07-07T09:00:03.936303Z","iopub.status.idle":"2024-07-07T09:01:38.223072Z","shell.execute_reply.started":"2024-07-07T09:00:03.936278Z","shell.execute_reply":"2024-07-07T09:01:38.221816Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.030832529067993164,\n 'eval_precision': 0.9502667983744018,\n 'eval_recall': 0.9484699918513616,\n 'eval_f1': 0.9493675449387823,\n 'eval_accuracy': 0.9904092466031396,\n 'eval_runtime': 94.2764,\n 'eval_samples_per_second': 129.736,\n 'eval_steps_per_second': 8.114,\n 'epoch': 10.0}"},"metadata":{}}]},{"cell_type":"code","source":"config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"query\",\"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_save=[\"classifier\",\"query\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T18:02:36.738384Z","iopub.execute_input":"2024-07-06T18:02:36.738861Z","iopub.status.idle":"2024-07-06T18:02:36.746503Z","shell.execute_reply.started":"2024-07-06T18:02:36.738828Z","shell.execute_reply":"2024-07-06T18:02:36.744384Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"def get_incorrect_word(model , sentence):\n    tokenized_text = word_tokenize(sentence)\n    #print(tokenized_text)\n    tokenized_inputs = Tokenizer(tokenized_text, truncation=True ,is_split_into_words=True)\n    \n    input_ids = torch.tensor(tokenized_inputs['input_ids']).unsqueeze(0)\n    \n    prediction = model(input_ids).logits.argmax(2)[0]\n    \n    #print(prediction)\n    \n    word_ids = tokenized_inputs.word_ids()\n    \n    previous_word_idx = None\n    \n    true_predictions = []\n    \n    for i,word_idx in enumerate(word_ids):\n        #print(word_idx)\n        if word_idx != None and word_idx != previous_word_idx:\n            true_predictions.append(prediction[i].item())\n        previous_word_idx = word_idx\n        \n    true_predictions = torch.tensor(true_predictions)\n    \n    indices = torch.nonzero(true_predictions == 1, as_tuple=False).squeeze()\n\n    if indices.numel() == 0:\n        return \"No incorrect words found\"\n        \n    if indices.dim() == 0:  \n        indices = indices.unsqueeze(0)\n    \n    incorrect_words = [tokenized_text[i.item()] for i in indices]\n        \n    return incorrect_words","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:56:32.552453Z","iopub.execute_input":"2024-07-11T13:56:32.553266Z","iopub.status.idle":"2024-07-11T13:56:32.564287Z","shell.execute_reply.started":"2024-07-11T13:56:32.553223Z","shell.execute_reply":"2024-07-11T13:56:32.563175Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nReloaded_BERTModel = BertForTokenClassification.from_pretrained('/kaggle/working/checkpoint-12000')","metadata":{"execution":{"iopub.status.busy":"2024-07-07T09:01:56.333299Z","iopub.execute_input":"2024-07-07T09:01:56.334128Z","iopub.status.idle":"2024-07-07T09:01:56.499111Z","shell.execute_reply.started":"2024-07-07T09:01:56.334097Z","shell.execute_reply":"2024-07-07T09:01:56.498343Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"Reloaded_BERTModel.device","metadata":{"execution":{"iopub.status.busy":"2024-07-07T09:01:57.992913Z","iopub.execute_input":"2024-07-07T09:01:57.993726Z","iopub.status.idle":"2024-07-07T09:01:57.999121Z","shell.execute_reply.started":"2024-07-07T09:01:57.993694Z","shell.execute_reply":"2024-07-07T09:01:57.998223Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"get_incorrect_word(Reloaded_BERTModel , \"The car was collided by a truck\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T09:18:13.391711Z","iopub.execute_input":"2024-07-07T09:18:13.392378Z","iopub.status.idle":"2024-07-07T09:18:13.488590Z","shell.execute_reply.started":"2024-07-07T09:18:13.392348Z","shell.execute_reply":"2024-07-07T09:18:13.487526Z"},"trusted":true},"execution_count":113,"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"'No incorrect words found'"},"metadata":{}}]},{"cell_type":"code","source":"get_incorrect_word(Reloaded_BERTModel , \"The car has collided by a truck\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T09:18:21.842841Z","iopub.execute_input":"2024-07-07T09:18:21.843437Z","iopub.status.idle":"2024-07-07T09:18:21.920932Z","shell.execute_reply.started":"2024-07-07T09:18:21.843408Z","shell.execute_reply":"2024-07-07T09:18:21.919982Z"},"trusted":true},"execution_count":114,"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"['has']"},"metadata":{}}]},{"cell_type":"code","source":"# Still giving some wrong outputs, so we need further finetuning\nget_incorrect_word(Reloaded_BERTModel , \"The car collided a truck\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T09:16:46.739035Z","iopub.execute_input":"2024-07-07T09:16:46.739648Z","iopub.status.idle":"2024-07-07T09:16:46.806740Z","shell.execute_reply.started":"2024-07-07T09:16:46.739618Z","shell.execute_reply":"2024-07-07T09:16:46.805620Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"['collided']"},"metadata":{}}]},{"cell_type":"code","source":"get_incorrect_word(Reloaded_BERTModel , \"The cat ate two mice\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T09:23:37.973975Z","iopub.execute_input":"2024-07-07T09:23:37.974670Z","iopub.status.idle":"2024-07-07T09:23:38.042395Z","shell.execute_reply.started":"2024-07-07T09:23:37.974640Z","shell.execute_reply":"2024-07-07T09:23:38.041373Z"},"trusted":true},"execution_count":140,"outputs":[{"execution_count":140,"output_type":"execute_result","data":{"text/plain":"'No incorrect words found'"},"metadata":{}}]},{"cell_type":"markdown","source":"## WikiPedia-data for finetuning","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nData_wiki = pd.read_csv(\"/kaggle/input/wikipedia-datase-for-spell-correction/wiki_dataset_with_errors.csv\")\n\ntqdm.pandas(desc=\"Processing rows\")\n\nData_wiki[\"Label\"] = Data_wiki[['Final','Errors']].progress_apply(label_generator,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:26:49.685143Z","iopub.execute_input":"2024-07-11T11:26:49.685575Z","iopub.status.idle":"2024-07-11T11:28:46.151633Z","shell.execute_reply.started":"2024-07-11T11:26:49.685543Z","shell.execute_reply":"2024-07-11T11:28:46.150694Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Processing rows:   0%|          | 0/30000 [00:00<?, ?it/s]/tmp/ipykernel_34/969590983.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  c = word_tokenize(txt[0])\n/tmp/ipykernel_34/969590983.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  i = word_tokenize(txt[1])\nProcessing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [01:55<00:00, 260.59it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"consec_ones = []\nfor i,items in enumerate(Data_wiki[\"Label\"].values):\n    max_len = 0\n    counter = 0\n    for j in items:\n        if j==1:\n            counter+=1\n        else:\n            max_len = max(max_len,counter)\n            counter=0\n    max_len = max(max_len,counter)\n    consec_ones.append(max_len)\n\n\nimport numpy as np\narr = np.array(consec_ones)\nindices = np.where(arr > 3)[0]\nprint(len(indices))\n#print(indices)\n\"\"\"Data.drop(indices,inplace = True)\nData.reset_index(inplace = True,drop=True)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:28:46.153413Z","iopub.execute_input":"2024-07-11T11:28:46.153700Z","iopub.status.idle":"2024-07-11T11:28:48.146944Z","shell.execute_reply.started":"2024-07-11T11:28:46.153674Z","shell.execute_reply":"2024-07-11T11:28:48.146050Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"1514\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'Data.drop(indices,inplace = True)\\nData.reset_index(inplace = True,drop=True)'"},"metadata":{}}]},{"cell_type":"code","source":"Data_wiki.drop(indices,inplace = True)\nData_wiki.reset_index(inplace = True,drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:28:48.148161Z","iopub.execute_input":"2024-07-11T11:28:48.148459Z","iopub.status.idle":"2024-07-11T11:28:48.164084Z","shell.execute_reply.started":"2024-07-11T11:28:48.148435Z","shell.execute_reply":"2024-07-11T11:28:48.163365Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"Data_wiki.drop(12347,inplace = True)\nData_wiki.reset_index(inplace=True,drop=True)\n\nTrain_data = Data_wiki[:25000]\nValidation_data = Data_wiki[25000:]\n\nText_train = Train_data[\"Errors\"].values.tolist()\nLabel_train = Train_data[\"Label\"].values.tolist()\n\n\ndata_dict = {\"Text\":Text_train,\"Label\":Label_train}\nds_train = Dataset.from_dict(data_dict)\n\nText_val = Validation_data[\"Errors\"].values.tolist()\nLabel_val = Validation_data[\"Label\"].values.tolist()\n\ndata_dict = {\"Text\":Text_val,\"Label\":Label_val}\nds_val = Dataset.from_dict(data_dict)\n\ndataset = DatasetDict({\n    'train': ds_train,\n    'validation': ds_val\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:28:48.166257Z","iopub.execute_input":"2024-07-11T11:28:48.166597Z","iopub.status.idle":"2024-07-11T11:28:49.622934Z","shell.execute_reply.started":"2024-07-11T11:28:48.166566Z","shell.execute_reply":"2024-07-11T11:28:49.622093Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"new = dataset.map(tokenize_and_align_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:28:49.624025Z","iopub.execute_input":"2024-07-11T11:28:49.624347Z","iopub.status.idle":"2024-07-11T11:30:31.549340Z","shell.execute_reply.started":"2024-07-11T11:28:49.624322Z","shell.execute_reply":"2024-07-11T11:30:31.548393Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9075b75211bc4fd48d80d0a3dc4266f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3485 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cfd031239da46cb9832393814a0d4dc"}},"metadata":{}}]},{"cell_type":"code","source":"new[\"train\"] = new[\"train\"].remove_columns([\"Text\", \"Label\"])\nnew['validation'] = new['validation'].remove_columns([\"Text\", \"Label\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:31:56.920963Z","iopub.execute_input":"2024-07-10T06:31:56.921407Z","iopub.status.idle":"2024-07-10T06:31:56.933565Z","shell.execute_reply.started":"2024-07-10T06:31:56.921370Z","shell.execute_reply":"2024-07-10T06:31:56.932257Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#!pip install peft\nfrom peft import LoraModel, LoraConfig, get_peft_model, PeftModel\nfrom peft import PeftConfig\nimport copy\nconfig = LoraConfig(\n    task_type=\"TOKEN_CLS\",\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"query\",\"value\"],\n    lora_dropout=0.01,\n    modules_to_save = [\"classifier\"]\n)\nBERT = BertForTokenClassification.from_pretrained(\"/kaggle/input/bert-trained-spellcheck/Bert for spell check\")\nprint(print_trainable_parameters(BERT))\nlora_model = get_peft_model(copy.deepcopy(BERT), config, adapter_name = \"LORA\")\nprint_trainable_parameters(lora_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:31:08.994406Z","iopub.execute_input":"2024-07-11T11:31:08.994809Z","iopub.status.idle":"2024-07-11T11:31:09.362284Z","shell.execute_reply.started":"2024-07-11T11:31:08.994777Z","shell.execute_reply":"2024-07-11T11:31:09.361348Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"trainable params: 107721218 || all params: 107721218 || trainable%: 100.00\nNone\ntrainable params: 296450 || all params: 108017668 || trainable%: 0.27\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",         \n    num_train_epochs=10,              \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,    \n    warmup_steps=50,                \n    weight_decay=0.01,               \n    logging_dir=\"/kaggle/working/\",            \n    logging_steps=10,                \n    save_steps=500,                 \n    save_total_limit=3,              \n    eval_steps=500,                  \n    load_best_model_at_end=True, \n    evaluation_strategy = \"steps\",\n    report_to=[],\n)\n\n#BERT = BertForTokenClassification.from_pretrained(\"/kaggle/input/bert-trained-spellcheck/Bert for spell check\")\n\ntrainer = Trainer(\n    model=lora_model,                     \n    args=training_args,               \n    train_dataset=new[\"train\"],\n    eval_dataset = new[\"validation\"],\n    tokenizer=Tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:31:11.130677Z","iopub.execute_input":"2024-07-11T11:31:11.131450Z","iopub.status.idle":"2024-07-11T11:31:11.488572Z","shell.execute_reply.started":"2024-07-11T11:31:11.131417Z","shell.execute_reply":"2024-07-11T11:31:11.487794Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T11:31:14.974516Z","iopub.execute_input":"2024-07-11T11:31:14.974888Z","iopub.status.idle":"2024-07-11T13:53:04.736117Z","shell.execute_reply.started":"2024-07-11T11:31:14.974860Z","shell.execute_reply":"2024-07-11T13:53:04.735189Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7820' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7820/7820 2:21:47, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.032800</td>\n      <td>0.026721</td>\n      <td>0.913661</td>\n      <td>0.891519</td>\n      <td>0.902454</td>\n      <td>0.990250</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.027900</td>\n      <td>0.024128</td>\n      <td>0.921790</td>\n      <td>0.903936</td>\n      <td>0.912776</td>\n      <td>0.991305</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.025100</td>\n      <td>0.022724</td>\n      <td>0.926900</td>\n      <td>0.910067</td>\n      <td>0.918406</td>\n      <td>0.991887</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.022000</td>\n      <td>0.021825</td>\n      <td>0.928102</td>\n      <td>0.915869</td>\n      <td>0.921945</td>\n      <td>0.992242</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.024100</td>\n      <td>0.021251</td>\n      <td>0.931479</td>\n      <td>0.915743</td>\n      <td>0.923544</td>\n      <td>0.992417</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.019700</td>\n      <td>0.020818</td>\n      <td>0.933566</td>\n      <td>0.917397</td>\n      <td>0.925411</td>\n      <td>0.992608</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.022500</td>\n      <td>0.020437</td>\n      <td>0.934830</td>\n      <td>0.919118</td>\n      <td>0.926907</td>\n      <td>0.992760</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.022400</td>\n      <td>0.020119</td>\n      <td>0.935428</td>\n      <td>0.919824</td>\n      <td>0.927560</td>\n      <td>0.992829</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.018400</td>\n      <td>0.020033</td>\n      <td>0.935670</td>\n      <td>0.921555</td>\n      <td>0.928559</td>\n      <td>0.992921</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.022200</td>\n      <td>0.019722</td>\n      <td>0.934923</td>\n      <td>0.923450</td>\n      <td>0.929151</td>\n      <td>0.992977</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.020900</td>\n      <td>0.019867</td>\n      <td>0.938026</td>\n      <td>0.920501</td>\n      <td>0.929181</td>\n      <td>0.993007</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.020400</td>\n      <td>0.019279</td>\n      <td>0.937645</td>\n      <td>0.924388</td>\n      <td>0.930969</td>\n      <td>0.993171</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.021500</td>\n      <td>0.019436</td>\n      <td>0.937614</td>\n      <td>0.923605</td>\n      <td>0.930557</td>\n      <td>0.993138</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.019800</td>\n      <td>0.019211</td>\n      <td>0.938358</td>\n      <td>0.924601</td>\n      <td>0.931429</td>\n      <td>0.993221</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.021600</td>\n      <td>0.019286</td>\n      <td>0.937794</td>\n      <td>0.923847</td>\n      <td>0.930768</td>\n      <td>0.993149</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INCORRECT seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/bert-trained-spellcheck/Bert for spell check - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nCould not locate the best model at /kaggle/working/checkpoint-7000/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7820, training_loss=0.023149509611718185, metrics={'train_runtime': 8509.4053, 'train_samples_per_second': 29.379, 'train_steps_per_second': 0.919, 'total_flos': 4.669980666294086e+16, 'train_loss': 0.023149509611718185, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"lora_model.save_pretrained(\"lora_model_adapter\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data_wiki.loc[1900]","metadata":{"execution":{"iopub.status.busy":"2024-07-10T04:53:01.563145Z","iopub.execute_input":"2024-07-10T04:53:01.563536Z","iopub.status.idle":"2024-07-10T04:53:01.573926Z","shell.execute_reply.started":"2024-07-10T04:53:01.563506Z","shell.execute_reply":"2024-07-10T04:53:01.572527Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"Title              Ramakrishna Mission Sevashrama, Sargachi\nFinal     Ramakrishna Mission Sevashrama Sargachi is loc...\nErrors    Ramakrishna Mission Sevashrama Sargachi were l...\nLabel     [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\nName: 1900, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\n\nbase_model = BertForTokenClassification.from_pretrained(\"/kaggle/input/bert-trained-spellcheck/Bert for spell check\")\ninference_model = PeftModel.from_pretrained(copy.deepcopy(base_model), \"/kaggle/working/checkpoint-7000/LORA\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:56:58.415166Z","iopub.execute_input":"2024-07-11T13:56:58.415537Z","iopub.status.idle":"2024-07-11T13:56:58.849385Z","shell.execute_reply.started":"2024-07-11T13:56:58.415509Z","shell.execute_reply":"2024-07-11T13:56:58.848403Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"base_model","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:58:26.243502Z","iopub.execute_input":"2024-07-11T13:58:26.244189Z","iopub.status.idle":"2024-07-11T13:58:26.252164Z","shell.execute_reply.started":"2024-07-11T13:58:26.244141Z","shell.execute_reply":"2024-07-11T13:58:26.251227Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"inference_model.device","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:58:31.058164Z","iopub.execute_input":"2024-07-11T13:58:31.058521Z","iopub.status.idle":"2024-07-11T13:58:31.064516Z","shell.execute_reply.started":"2024-07-11T13:58:31.058495Z","shell.execute_reply":"2024-07-11T13:58:31.063574Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Lets test the model","metadata":{}},{"cell_type":"code","source":"import torch\nget_incorrect_word(inference_model , \"I am in the school now\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:26.905219Z","iopub.execute_input":"2024-07-11T14:03:26.906111Z","iopub.status.idle":"2024-07-11T14:03:26.970437Z","shell.execute_reply.started":"2024-07-11T14:03:26.906067Z","shell.execute_reply":"2024-07-11T14:03:26.969342Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"'No incorrect words found'"},"metadata":{}}]},{"cell_type":"code","source":"get_incorrect_word(inference_model , \"I am in the school yesterday\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:39.076864Z","iopub.execute_input":"2024-07-11T14:03:39.077228Z","iopub.status.idle":"2024-07-11T14:03:39.145462Z","shell.execute_reply.started":"2024-07-11T14:03:39.077199Z","shell.execute_reply":"2024-07-11T14:03:39.144343Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"['am']"},"metadata":{}}]},{"cell_type":"code","source":"get_incorrect_word(inference_model , \"I was in the school yesterday\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:26:54.590567Z","iopub.execute_input":"2024-07-11T14:26:54.591163Z","iopub.status.idle":"2024-07-11T14:26:54.678538Z","shell.execute_reply.started":"2024-07-11T14:26:54.591131Z","shell.execute_reply":"2024-07-11T14:26:54.677599Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"'No incorrect words found'"},"metadata":{}}]},{"cell_type":"code","source":"get_incorrect_word(inference_model , \"I am in the school tomorrow\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:16:46.783654Z","iopub.execute_input":"2024-07-11T14:16:46.784006Z","iopub.status.idle":"2024-07-11T14:16:46.885982Z","shell.execute_reply.started":"2024-07-11T14:16:46.783979Z","shell.execute_reply":"2024-07-11T14:16:46.885059Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"['am']"},"metadata":{}}]},{"cell_type":"code","source":"print(get_incorrect_word(inference_model , \"I will be in the school tomorrow\"))\nprint(get_incorrect_word(inference_model , \"I shall be in the school tomorrow\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:17:13.360852Z","iopub.execute_input":"2024-07-11T14:17:13.361338Z","iopub.status.idle":"2024-07-11T14:17:13.494066Z","shell.execute_reply.started":"2024-07-11T14:17:13.361303Z","shell.execute_reply":"2024-07-11T14:17:13.493008Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"No incorrect words found\nNo incorrect words found\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"\"\"On this page you will find our complete \nlist of high quality reading comprehension \nworksheets created specially by our team for \nstudents in grade levels K-12. Our worksheets \nelicit the use of critical thinking skills at \nevery level. While some questions ask the reader \nto peruse the passage for particular details, \nmost questions involve the use of deductive reasoning, \nconclusion making, logical inference, sequential analysis, \ntonal awareness, and an understanding of scope.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:22:08.712008Z","iopub.execute_input":"2024-07-11T14:22:08.712438Z","iopub.status.idle":"2024-07-11T14:22:08.717527Z","shell.execute_reply.started":"2024-07-11T14:22:08.712406Z","shell.execute_reply":"2024-07-11T14:22:08.716456Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"get_incorrect_word(inference_model , text)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:22:19.030050Z","iopub.execute_input":"2024-07-11T14:22:19.030436Z","iopub.status.idle":"2024-07-11T14:22:19.224302Z","shell.execute_reply.started":"2024-07-11T14:22:19.030407Z","shell.execute_reply":"2024-07-11T14:22:19.223298Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"'No incorrect words found'"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\nchanges: \n\nfind -> found\nstudents -> studentsss\ngrade -> graded\ninference -> inferenced\nsequentiallllyyy -> sequential\ntonal -> ton\nunderstanding -> understood\n\n\"\"\"\n\n\nincorrect_text = \"\"\"On this page you will found our complete \nlist of high quality reading comprehension \nworksheets created specially by our team for \nstudentsss in graded leveled K-12. Our worksheets \nelicit the use of critical thinking skills at \nevery level. While some questions ask the reader \nto peruse the passage for particular details, \nmost questions involve the use of deductive reasoning, \nconclusion making, logical inferenced, sequentiallllyyy analysis, \nton awareness, and an understood of scope.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:24:04.955006Z","iopub.execute_input":"2024-07-11T14:24:04.955891Z","iopub.status.idle":"2024-07-11T14:24:04.960727Z","shell.execute_reply.started":"2024-07-11T14:24:04.955856Z","shell.execute_reply":"2024-07-11T14:24:04.959461Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"get_incorrect_word(inference_model , incorrect_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:23:45.888878Z","iopub.execute_input":"2024-07-11T14:23:45.889740Z","iopub.status.idle":"2024-07-11T14:23:46.032362Z","shell.execute_reply.started":"2024-07-11T14:23:45.889705Z","shell.execute_reply":"2024-07-11T14:23:46.031389Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"['found',\n 'studentsss',\n 'graded',\n 'leveled',\n 'inferenced',\n 'sequentiallllyyy',\n 'ton',\n 'understood']"},"metadata":{}}]}]}